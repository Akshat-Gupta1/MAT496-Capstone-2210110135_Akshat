{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Research Analyzer - Multi-Analyst News Research System\n",
                "\n",
                "## Phase 1: Setup & Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "%%capture --no-stderr\n",
                "%pip install --quiet -U langgraph langchain_openai langchain_community langchain_core tavily-python python-dotenv pydantic"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import os, getpass\n",
                "def _set_env(var: str):\n",
                "    if not os.environ.get(var):\n",
                "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
                "_set_env(\"OPENAI_API_KEY\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "_set_env(\"TAVILY_API_KEY\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "from langchain_openai import ChatOpenAI\n",
                "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 2: Data Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "from typing import List\n",
                "from pydantic import BaseModel, Field\n",
                "\n",
                "class NewsAnalyst(BaseModel):\n",
                "    affiliation: str = Field(description=\"Primary affiliation of the analyst.\")\n",
                "    name: str = Field(description=\"Name of the analyst.\")\n",
                "    role: str = Field(description=\"Role of the analyst.\")\n",
                "    description: str = Field(description=\"Description of the analyst's focus and expertise.\")\n",
                "    @property\n",
                "    def persona(self) -> str:\n",
                "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
                "\n",
                "class AnalystTeam(BaseModel):\n",
                "    analysts: List[NewsAnalyst] = Field(description=\"Team of news analysts.\")\n",
                "\n",
                "class SearchQuery(BaseModel):\n",
                "    search_query: str = Field(None, description=\"Search query for news retrieval.\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 3: State Definitions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "import operator\n",
                "from typing import Annotated\n",
                "from typing_extensions import TypedDict\n",
                "from langgraph.graph import MessagesState\n",
                "\n",
                "class GenerateAnalystsState(TypedDict):\n",
                "    topic: str\n",
                "    max_analysts: int\n",
                "    human_analyst_feedback: str\n",
                "    analysts: List[NewsAnalyst]\n",
                "\n",
                "class AnalysisState(MessagesState):\n",
                "    max_num_turns: int\n",
                "    context: Annotated[list, operator.add]\n",
                "    analyst: NewsAnalyst\n",
                "    analysis: str\n",
                "    sections: Annotated[list, operator.add]\n",
                "\n",
                "class ResearchGraphState(TypedDict):\n",
                "    topic: str\n",
                "    max_analysts: int\n",
                "    human_analyst_feedback: str\n",
                "    analysts: List[NewsAnalyst]\n",
                "    sections: Annotated[list, operator.add]\n",
                "    introduction: str\n",
                "    content: str\n",
                "    conclusion: str\n",
                "    final_report: str"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 4: Prompt Templates"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "ANALYST_INSTRUCTIONS = \"\"\"You are tasked with creating a team of specialized news analysts.\n",
                "1. Review the topic: {topic}\n",
                "2. Examine any editorial feedback: {human_analyst_feedback}\n",
                "3. Determine the most important perspectives for comprehensive news analysis.\n",
                "4. Pick the top {max_analysts} perspectives.\n",
                "5. Assign one analyst to each perspective with relevant expertise.\"\"\"\n",
                "\n",
                "QUESTION_INSTRUCTIONS = \"\"\"You are a news analyst conducting research on {topic}.\n",
                "Your goal is to gather specific, actionable insights. Your analytical focus: {goals}\n",
                "Begin by introducing yourself, then pose your analytical questions.\n",
                "When satisfied, conclude with: \\\"Analysis complete!\\\"\"\"\"\n",
                "\n",
                "SEARCH_INSTRUCTIONS = \"\"\"Generate a search query for recent news and information.\n",
                "Focus on the latest developments relevant to the conversation.\"\"\"\n",
                "\n",
                "ANSWER_INSTRUCTIONS = \"\"\"You are a news information expert.\n",
                "Analyst focus: {goals}\n",
                "Answer using this context: {context}\n",
                "Guidelines: Use only provided context, include specific data, cite sources [1], [2], etc.\"\"\"\n",
                "\n",
                "SECTION_WRITER_INSTRUCTIONS = \"\"\"You are a news report writer.\n",
                "Create a concise section based on analyst research.\n",
                "Structure: ## {focus} (title), ### Key Findings, ### Analysis, ### Sources\n",
                "Maximum 300 words. Use numbered sources.\"\"\"\n",
                "\n",
                "REPORT_WRITER_INSTRUCTIONS = \"\"\"You are creating a comprehensive news report on: {topic}\n",
                "Task: Review all analyst sections, identify key insights, synthesize into cohesive narrative.\n",
                "Format: Use markdown, start with ## News Analysis, preserve citations, create ## Sources section.\n",
                "Analyst sections: {context}\"\"\"\n",
                "\n",
                "INTRODUCTION_INSTRUCTIONS = \"\"\"Write a compelling introduction for an analysis report on {topic}.\n",
                "Target 100 words. Use markdown. Create # title, then ## Introduction section.\n",
                "Report sections: {formatted_str_sections}\"\"\"\n",
                "\n",
                "CONCLUSION_INSTRUCTIONS = \"\"\"Write a conclusion for an analysis report on {topic}.\n",
                "Target 100 words. Use markdown. Use ## Conclusion header.\n",
                "Report sections: {formatted_str_sections}\"\"\""
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 5: Analyst Generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "from IPython.display import Image, display\n",
                "from langgraph.graph import START, END, StateGraph\n",
                "from langgraph.checkpoint.memory import MemorySaver\n",
                "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
                "\n",
                "def create_analysts(state: GenerateAnalystsState):\n",
                "    topic = state['topic']\n",
                "    max_analysts = state['max_analysts']\n",
                "    human_analyst_feedback = state.get('human_analyst_feedback', '')\n",
                "    structured_llm = llm.with_structured_output(AnalystTeam)\n",
                "    system_message = ANALYST_INSTRUCTIONS.format(topic=topic, human_analyst_feedback=human_analyst_feedback, max_analysts=max_analysts)\n",
                "    analysts = structured_llm.invoke([SystemMessage(content=system_message)] + [HumanMessage(content=\"Generate the analyst team.\")])\n",
                "    return {\"analysts\": analysts.analysts}\n",
                "\n",
                "def human_feedback(state: GenerateAnalystsState):\n",
                "    pass\n",
                "\n",
                "def should_continue(state: GenerateAnalystsState):\n",
                "    if state.get('human_analyst_feedback', None):\n",
                "        return \"create_analysts\"\n",
                "    return END"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 6: Analysis Workflow\n",
                "\n",
                "Create the search, question, and answer nodes for analyst research."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "from langchain_community.tools.tavily_search import TavilySearchResults\n",
                "from langchain_core.messages import get_buffer_string\n",
                "\n",
                "tavily_search = TavilySearchResults(max_results=3)\n",
                "\n",
                "def generate_question(state: AnalysisState):\n",
                "    analyst = state[\"analyst\"]\n",
                "    messages = state[\"messages\"]\n",
                "    first_msg = messages[0].content if messages else \"the topic\"\n",
                "    topic = first_msg.replace(\"Analyze \", \"\").replace(\" from your perspective.\", \"\") if \"Analyze \" in first_msg else \"the topic\"\n",
                "    system_message = QUESTION_INSTRUCTIONS.format(topic=topic, goals=analyst.persona)\n",
                "    question = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
                "    return {\"messages\": [question]}\n",
                "\n",
                "print(\"‚úì generate_question defined!\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def search_news(state: AnalysisState):\n",
                "    structured_llm = llm.with_structured_output(SearchQuery)\n",
                "    search_query = structured_llm.invoke([SystemMessage(content=SEARCH_INSTRUCTIONS)] + state['messages'])\n",
                "    try:\n",
                "        search_docs = tavily_search.invoke(search_query.search_query)\n",
                "    except Exception:\n",
                "        search_docs = []\n",
                "    if not search_docs:\n",
                "        formatted_search_docs = \"\"\n",
                "    else:\n",
                "        formatted_search_docs = \"\\n\\n---\\n\\n\".join([f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>' for doc in search_docs])\n",
                "    return {\"context\": [formatted_search_docs]}\n",
                "\n",
                "print(\"‚úì search_news defined!\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def generate_answer(state: AnalysisState):\n",
                "    analyst = state[\"analyst\"]\n",
                "    messages = state[\"messages\"]\n",
                "    context = state.get(\"context\", [])\n",
                "    system_message = ANSWER_INSTRUCTIONS.format(goals=analyst.persona, context=context)\n",
                "    answer = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
                "    answer.name = \"expert\"\n",
                "    return {\"messages\": [answer]}\n",
                "\n",
                "def save_analysis(state: AnalysisState):\n",
                "    messages = state[\"messages\"]\n",
                "    analysis = get_buffer_string(messages)\n",
                "    return {\"analysis\": analysis}\n",
                "\n",
                "def route_messages(state: AnalysisState, name: str = \"expert\"):\n",
                "    messages = state[\"messages\"]\n",
                "    max_num_turns = state.get('max_num_turns', 2)\n",
                "    num_responses = len([m for m in messages if isinstance(m, AIMessage) and m.name == name])\n",
                "    if num_responses >= max_num_turns:\n",
                "        return 'save_analysis'\n",
                "    last_question = messages[-2]\n",
                "    if \"Analysis complete\" in last_question.content:\n",
                "        return 'save_analysis'\n",
                "    return \"ask_question\"\n",
                "\n",
                "print(\"‚úì generate_answer, save_analysis, route_messages defined!\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def write_section(state: AnalysisState):\n",
                "    context = state.get(\"context\", [])\n",
                "    analyst = state[\"analyst\"]\n",
                "    system_message = SECTION_WRITER_INSTRUCTIONS.format(focus=analyst.description)\n",
                "    section = llm.invoke([SystemMessage(content=system_message)] + [HumanMessage(content=f\"Use this research: {context}\")])\n",
                "    return {\"sections\": [section.content]}\n",
                "\n",
                "print(\"‚úì write_section defined!\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Build analysis subgraph\n",
                "analysis_builder = StateGraph(AnalysisState)\n",
                "analysis_builder.add_node(\"ask_question\", generate_question)\n",
                "analysis_builder.add_node(\"search_news\", search_news)\n",
                "analysis_builder.add_node(\"answer_question\", generate_answer)\n",
                "analysis_builder.add_node(\"save_analysis\", save_analysis)\n",
                "analysis_builder.add_node(\"write_section\", write_section)\n",
                "\n",
                "analysis_builder.add_edge(START, \"ask_question\")\n",
                "analysis_builder.add_edge(\"ask_question\", \"search_news\")\n",
                "analysis_builder.add_edge(\"search_news\", \"answer_question\")\n",
                "analysis_builder.add_conditional_edges(\"answer_question\", route_messages, ['ask_question', 'save_analysis'])\n",
                "analysis_builder.add_edge(\"save_analysis\", \"write_section\")\n",
                "analysis_builder.add_edge(\"write_section\", END)\n",
                "\n",
                "print(\"‚úì Analysis subgraph built!\")\n",
                "display(Image(analysis_builder.compile().get_graph().draw_mermaid_png()))"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 7: Report Writing\n",
                "\n",
                "Create report writing nodes (introduction, body, conclusion)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "from langgraph.types import Send\n",
                "\n",
                "def initiate_all_analyses(state: ResearchGraphState):\n",
                "    human_analyst_feedback = state.get('human_analyst_feedback')\n",
                "    if human_analyst_feedback:\n",
                "        return \"create_analysts\"\n",
                "    topic = state[\"topic\"]\n",
                "    return [Send(\"conduct_analysis\", {\"analyst\": analyst, \"messages\": [HumanMessage(content=f\"Analyze {topic} from your perspective.\")]}) for analyst in state[\"analysts\"]]\n",
                "\n",
                "print(\"‚úì initiate_all_analyses defined!\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def write_report(state: ResearchGraphState):\n",
                "    sections = state[\"sections\"]\n",
                "    topic = state[\"topic\"]\n",
                "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
                "    system_message = REPORT_WRITER_INSTRUCTIONS.format(topic=topic, context=formatted_str_sections)\n",
                "    report = llm.invoke([SystemMessage(content=system_message)] + [HumanMessage(content=\"Write the news analysis report.\")])\n",
                "    return {\"content\": report.content}\n",
                "\n",
                "def write_introduction(state: ResearchGraphState):\n",
                "    sections = state[\"sections\"]\n",
                "    topic = state[\"topic\"]\n",
                "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
                "    instructions = INTRODUCTION_INSTRUCTIONS.format(topic=topic, formatted_str_sections=formatted_str_sections)\n",
                "    intro = llm.invoke([instructions] + [HumanMessage(content=\"Write the report introduction\")])\n",
                "    return {\"introduction\": intro.content}\n",
                "\n",
                "def write_conclusion(state: ResearchGraphState):\n",
                "    sections = state[\"sections\"]\n",
                "    topic = state[\"topic\"]\n",
                "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
                "    instructions = CONCLUSION_INSTRUCTIONS.format(topic=topic, formatted_str_sections=formatted_str_sections)\n",
                "    conclusion = llm.invoke([instructions] + [HumanMessage(content=\"Write the report conclusion\")])\n",
                "    return {\"conclusion\": conclusion.content}\n",
                "\n",
                "print(\"‚úì write_report, write_introduction, write_conclusion defined!\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def finalize_report(state: ResearchGraphState):\n",
                "    content = state[\"content\"]\n",
                "    if content.startswith(\"## News Analysis\"):\n",
                "        content = content.strip(\"## News Analysis\")\n",
                "    if \"## Sources\" in content:\n",
                "        try:\n",
                "            content, sources = content.split(\"\\n## Sources\\n\")\n",
                "        except:\n",
                "            sources = None\n",
                "    else:\n",
                "        sources = None\n",
                "    final_report = state[\"introduction\"] + \"\\n\\n---\\n\\n\" + content + \"\\n\\n---\\n\\n\" + state[\"conclusion\"]\n",
                "    if sources is not None:\n",
                "        final_report += \"\\n\\n## Sources\\n\" + sources\n",
                "    return {\"final_report\": final_report}\n",
                "\n",
                "print(\"‚úì finalize_report defined!\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 8: Full Pipeline\n",
                "\n",
                "Assemble the complete research graph and run a demo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Build the main research graph\n",
                "builder = StateGraph(ResearchGraphState)\n",
                "builder.add_node(\"create_analysts\", create_analysts)\n",
                "builder.add_node(\"human_feedback\", human_feedback)\n",
                "builder.add_node(\"conduct_analysis\", analysis_builder.compile())\n",
                "builder.add_node(\"write_report\", write_report)\n",
                "builder.add_node(\"write_introduction\", write_introduction)\n",
                "builder.add_node(\"write_conclusion\", write_conclusion)\n",
                "builder.add_node(\"finalize_report\", finalize_report)\n",
                "\n",
                "builder.add_edge(START, \"create_analysts\")\n",
                "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
                "builder.add_conditional_edges(\"human_feedback\", initiate_all_analyses, [\"create_analysts\", \"conduct_analysis\"])\n",
                "builder.add_edge(\"conduct_analysis\", \"write_report\")\n",
                "builder.add_edge(\"conduct_analysis\", \"write_introduction\")\n",
                "builder.add_edge(\"conduct_analysis\", \"write_conclusion\")\n",
                "builder.add_edge([\"write_conclusion\", \"write_report\", \"write_introduction\"], \"finalize_report\")\n",
                "builder.add_edge(\"finalize_report\", END)\n",
                "\n",
                "memory = MemorySaver()\n",
                "graph = builder.compile(checkpointer=memory)\n",
                "\n",
                "print(\"‚úì Full research graph compiled!\")\n",
                "display(Image(graph.get_graph().draw_mermaid_png()))"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Run the complete research pipeline\n",
                "from IPython.display import Markdown\n",
                "\n",
                "max_analysts = 3\n",
                "topic = \"SpaceX Starship Development\"\n",
                "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
                "\n",
                "final_report = None\n",
                "print(f\"üîç Starting research on: {topic}\")\n",
                "print(f\"üìä Using {max_analysts} analysts\\n\")\n",
                "\n",
                "for event in graph.stream({\"topic\": topic, \"max_analysts\": max_analysts}, thread, stream_mode=\"updates\"):\n",
                "    print(f\"  ‚úì Processing: {list(event.keys())}\")\n",
                "    if \"finalize_report\" in event:\n",
                "        final_report = event[\"finalize_report\"][\"final_report\"]\n",
                "\n",
                "print(\"\\n‚úÖ Research complete!\\n\")\n",
                "if final_report:\n",
                "    display(Markdown(final_report))"
            ],
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}